"""
ç®¡ç†å‘˜å›¾ç‰‡ç®¡ç†API - ç‹¬ç«‹æ¨¡å—
"""
from app.services.storage_service import storage_manager
from fastapi import APIRouter, Depends, HTTPException, Query, BackgroundTasks
from fastapi.responses import JSONResponse, StreamingResponse
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, func, text, desc
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
import json
import os
import io
import csv
import asyncio

from app.database import get_db
from app.auth.dependencies import require_admin
from app.models.user import User
from app.models.image import Image, Tag, ImageTag
from app.services.database_service import DatabaseService
from app.services.gpt4o_service import gpt4o_analyzer
from app.services.storage_service import storage_manager

router = APIRouter()


@router.get("/list")
async def get_images_list(
    page: int = Query(1, ge=1, description="é¡µç "),
    per_page: int = Query(15, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    status: Optional[str] = Query("active", description="çŠ¶æ€ç­›é€‰: active, deleted, all"),
    ai_status: Optional[str] = Query(None, description="AIåˆ†æçŠ¶æ€: pending, completed, failed"),
    uploader: Optional[str] = Query(None, description="ä¸Šä¼ è€…ç­›é€‰"),
    search: Optional[str] = Query(None, description="æœç´¢å…³é”®è¯"),
    sort_by: str = Query("upload_time", description="æ’åºå­—æ®µ"),
    sort_order: str = Query("desc", description="æ’åºæ–¹å‘: asc, desc"),
    current_user: User = Depends(require_admin),
    db: Session = Depends(get_db)
):
    """è·å–å›¾ç‰‡ç®¡ç†åˆ—è¡¨"""
    try:
        offset = (page - 1) * per_page
        
        # æ„å»ºæŸ¥è¯¢
        query = db.query(Image)
        
        # çŠ¶æ€ç­›é€‰
        if status == "active":
            query = query.filter(Image.is_active == True)
        elif status == "deleted":
            query = query.filter(Image.is_active == False)
        # "all" ä¸æ·»åŠ è¿‡æ»¤æ¡ä»¶
        
        # AIåˆ†æçŠ¶æ€ç­›é€‰
        if ai_status:
            if ai_status == "failed":
                query = query.filter(Image.ai_analysis_status == 'failed')
            elif ai_status == "pending":
                query = query.filter(Image.ai_analysis_status == 'pending')
            elif ai_status == "completed":
                query = query.filter(Image.ai_analysis_status == 'completed')
        
        # ä¸Šä¼ è€…ç­›é€‰
        if uploader:
            query = query.filter(Image.uploader.contains(uploader))
        
        # æœç´¢
        if search:
            query = query.filter(
                or_(
                    Image.filename.contains(search),
                    Image.ai_description.contains(search),
                    Image.uploader.contains(search)
                )
            )
        
        # æ’åº
        sort_column = getattr(Image, sort_by, Image.upload_time)
        if sort_order == "desc":
            query = query.order_by(desc(sort_column))
        else:
            query = query.order_by(sort_column)
        
        # æ€»æ•°
        total = query.count()
        
        # åˆ†é¡µæŸ¥è¯¢
        images = query.offset(offset).limit(per_page).all()
        
        # å¤„ç†ç»“æœ
        db_service = DatabaseService(db)
        result_images = []
        
        for image in images:
            tags = db_service.get_image_tags(image.id)
            
            # è§£ææœç´¢å…³é”®è¯
            searchable_keywords = []
            if hasattr(image, 'ai_searchable_keywords') and image.ai_searchable_keywords:
                try:
                    if isinstance(image.ai_searchable_keywords, str):
                        searchable_keywords = json.loads(image.ai_searchable_keywords)
                    else:
                        searchable_keywords = image.ai_searchable_keywords
                except:
                    pass
            
            result_images.append({
                "id": image.id,
                "filename": image.filename,
                "url": storage_manager.get_image_url(image.file_path),
                "thumbnail_url": storage_manager.get_image_url(image.file_path),
                "width": image.width,
                "height": image.height,
                "file_size": image.file_size,
                "description": image.ai_description,
                "confidence": image.ai_confidence,
                "ai_analysis_status": image.ai_analysis_status,
                "ai_model": getattr(image, 'ai_model', ''),
                "mood": getattr(image, 'ai_mood', ''),
                "style": getattr(image, 'ai_style', ''),
                "upload_time": image.upload_time.isoformat(),
                "view_count": image.view_count,
                "uploader": image.uploader,
                "is_active": image.is_active,
                "tags": [{"id": tag.id, "name": tag.name, "category": tag.category} for tag in tags],
                "searchable_keywords": searchable_keywords
            })
        
        return {
            "success": True,
            "data": {
                "images": result_images,
                "pagination": {
                    "page": page,
                    "per_page": per_page,
                    "total": total,
                    "pages": (total + per_page - 1) // per_page
                }
            }
        }
        
    except Exception as e:
        print(f"âŒ è·å–å›¾ç‰‡åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å›¾ç‰‡åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.get("/{image_id}/details")
async def get_image_details(
    image_id: int,
    current_user: User = Depends(require_admin),
    db: Session = Depends(get_db)
):
    """è·å–å›¾ç‰‡è¯¦ç»†ä¿¡æ¯"""
    try:
        db_service = DatabaseService(db)
        image = db_service.get_image_by_id(image_id, include_deleted=True)
        
        if not image:
            raise HTTPException(status_code=404, detail="å›¾ç‰‡ä¸å­˜åœ¨")
        
        tags = db_service.get_image_tags(image_id)
        
        # è§£æåŸå§‹åˆ†æç»“æœ
        raw_analysis = None
        searchable_keywords = []
        
        if hasattr(image, 'ai_analysis_raw') and image.ai_analysis_raw:
            try:
                if isinstance(image.ai_analysis_raw, str):
                    raw_analysis = json.loads(image.ai_analysis_raw)
                else:
                    raw_analysis = image.ai_analysis_raw
            except:
                pass
        
        if hasattr(image, 'ai_searchable_keywords') and image.ai_searchable_keywords:
            try:
                if isinstance(image.ai_searchable_keywords, str):
                    searchable_keywords = json.loads(image.ai_searchable_keywords)
                else:
                    searchable_keywords = image.ai_searchable_keywords
            except:
                pass
        
        return {
            "success": True,
            "data": {
                "id": image.id,
                "filename": image.filename,
                "url": storage_manager.get_image_url(image.file_path),
                "width": image.width,
                "height": image.height,
                "file_size": image.file_size,
                "description": image.ai_description,
                "confidence": image.ai_confidence,
                "ai_analysis_status": image.ai_analysis_status,
                "ai_model": getattr(image, 'ai_model', ''),
                "mood": getattr(image, 'ai_mood', ''),
                "style": getattr(image, 'ai_style', ''),
                "upload_time": image.upload_time.isoformat(),
                "view_count": image.view_count,
                "uploader": image.uploader,
                "is_active": image.is_active,
                "tags": [{"id": tag.id, "name": tag.name, "category": tag.category} for tag in tags],
                "searchable_keywords": searchable_keywords,
                "raw_analysis": raw_analysis
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å›¾ç‰‡è¯¦æƒ…å¤±è´¥: {str(e)}")


@router.put("/{image_id}")
async def update_image(
    image_id: int,
    description: Optional[str] = None,
    is_active: Optional[bool] = None,
    custom_tags: Optional[List[str]] = None,
    current_user: User = Depends(require_admin),
    db: Session = Depends(get_db)
):
    """æ›´æ–°å›¾ç‰‡ä¿¡æ¯"""
    try:
        db_service = DatabaseService(db)
        image = db_service.get_image_by_id(image_id, include_deleted=True)
        
        if not image:
            raise HTTPException(status_code=404, detail="å›¾ç‰‡ä¸å­˜åœ¨")
        
        # æ›´æ–°åŸºæœ¬ä¿¡æ¯
        if description is not None:
            image.ai_description = description
        
        if is_active is not None:
            image.is_active = is_active
        
        # æ›´æ–°è‡ªå®šä¹‰æ ‡ç­¾
        if custom_tags is not None:
            # åˆ é™¤ç°æœ‰æ ‡ç­¾
            db.query(ImageTag).filter(ImageTag.image_id == image_id).delete()
            
            # æ·»åŠ æ–°æ ‡ç­¾
            if custom_tags:
                db_service.add_tags_to_image(image_id, custom_tags, 'admin', [0.9] * len(custom_tags))
        
        db.commit()
        
        return {
            "success": True,
            "message": "å›¾ç‰‡ä¿¡æ¯æ›´æ–°æˆåŠŸ"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å›¾ç‰‡ä¿¡æ¯å¤±è´¥: {str(e)}")


@router.post("/{image_id}/reanalyze")
async def reanalyze_image(
    image_id: int,
    background_tasks: BackgroundTasks,
    custom_prompt: Optional[str] = None,
    current_user: User = Depends(require_admin),
    db: Session = Depends(get_db)
):
    """é‡æ–°åˆ†æå›¾ç‰‡"""
    try:
        print(f"ğŸ”„ æ”¶åˆ°é‡æ–°åˆ†æè¯·æ±‚ - å›¾ç‰‡ID: {image_id}, ç”¨æˆ·: {current_user.username}")
        
        db_service = DatabaseService(db)
        image = db_service.get_image_by_id(image_id, include_deleted=True)
        
        if not image:
            print(f"âŒ å›¾ç‰‡ä¸å­˜åœ¨ - ID: {image_id}")
            raise HTTPException(status_code=404, detail="å›¾ç‰‡ä¸å­˜åœ¨")
        
        print(f"ğŸ“„ å›¾ç‰‡ä¿¡æ¯ - æ–‡ä»¶å: {image.filename}, è·¯å¾„: {image.file_path}, å½“å‰çŠ¶æ€: {image.ai_analysis_status}")
        
        # æ›´æ–°çŠ¶æ€ä¸ºé‡æ–°åˆ†æä¸­
        try:
            image.ai_analysis_status = 'pending'
            if custom_prompt:
                image.ai_model = 'gpt-4o-custom'
            db.commit()
            print(f"âœ… çŠ¶æ€æ›´æ–°ä¸ºpending")
        except Exception as status_error:
            print(f"âŒ æ›´æ–°çŠ¶æ€å¤±è´¥: {status_error}")
            db.rollback()
            raise HTTPException(status_code=500, detail=f"æ›´æ–°çŠ¶æ€å¤±è´¥: {str(status_error)}")
        
        print(f"ğŸš€ å¯åŠ¨åå°åˆ†æä»»åŠ¡")
        
        # å¯åŠ¨é‡æ–°åˆ†æä»»åŠ¡
        background_tasks.add_task(
            reanalyze_image_task, 
            image_id, 
            image.file_path,
            custom_prompt
        )
        
        return {
            "success": True,
            "message": "å·²å¯åŠ¨é‡æ–°åˆ†æä»»åŠ¡"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ å¯åŠ¨é‡æ–°åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨é‡æ–°åˆ†æå¤±è´¥: {str(e)}")
    
async def batch_reanalyze_task(image_ids: List[int], custom_prompt: Optional[str] = None):
    """æ‰¹é‡é‡æ–°åˆ†æä»»åŠ¡"""
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡é‡æ–°åˆ†æ {len(image_ids)} å¼ å›¾ç‰‡")
    
    success_count = 0
    failed_count = 0
    
    for i, image_id in enumerate(image_ids):
        try:
            from app.database import SessionLocal
            db = SessionLocal()
            
            image = db.query(Image).filter(Image.id == image_id).first()
            if image:
                print(f"ğŸ“Š åˆ†æè¿›åº¦: {i+1}/{len(image_ids)} - å›¾ç‰‡ID: {image_id}")
                
                # ä¿®å¤ï¼šè·å–å®Œæ•´çš„OSS URL
                from app.services.storage_service import StorageManager
                storage_manager = StorageManager()
                image_url = storage_manager.get_oss_url(image.file_path)
                
                # è°ƒç”¨é‡æ–°åˆ†æä»»åŠ¡ - ä¼ é€’å®Œæ•´URL
                await reanalyze_image_task(image_id, image.file_path, None)
                
                # æ£€æŸ¥ç»“æœ
                db.refresh(image)
                if image.ai_analysis_status == 'completed':
                    success_count += 1
                else:
                    failed_count += 1
                
                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                await asyncio.sleep(2)
            
            db.close()
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡åˆ†æå›¾ç‰‡ {image_id} å¤±è´¥: {e}")
            failed_count += 1
            continue
    
    print(f"âœ… æ‰¹é‡é‡æ–°åˆ†æå®Œæˆ - æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")


@router.post("/batch-reanalyze")
async def batch_reanalyze_images(
    background_tasks: BackgroundTasks,
    image_ids: List[int],
    status_filter: str = Query("failed", description="åˆ†æçŠ¶æ€ç­›é€‰: pending, failed, all"),
    custom_prompt: Optional[str] = None,
    current_user: User = Depends(require_admin),
    db: Session = Depends(get_db)
):
    """æ‰¹é‡é‡æ–°åˆ†æå›¾ç‰‡"""
    try:
        print(f"ğŸš€ æ‰¹é‡é‡æ–°åˆ†æè¯·æ±‚ - å›¾ç‰‡IDs: {image_ids}, çŠ¶æ€ç­›é€‰: {status_filter}")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå›¾ç‰‡IDï¼Œæ ¹æ®çŠ¶æ€ç­›é€‰æŸ¥æ‰¾
        if not image_ids:
            query = db.query(Image).filter(Image.is_active == True)
            
            if status_filter == "failed":
                query = query.filter(Image.ai_analysis_status == 'failed')
            elif status_filter == "pending":
                query = query.filter(Image.ai_analysis_status == 'pending')
            elif status_filter == "all":
                query = query.filter(Image.ai_analysis_status.in_(['failed', 'pending']))
            
            images = query.limit(50).all()  # é™åˆ¶æ‰¹é‡æ•°é‡
            image_ids = [img.id for img in images]
        else:
            # éªŒè¯æŒ‡å®šçš„å›¾ç‰‡ID
            images = db.query(Image).filter(
                and_(
                    Image.id.in_(image_ids),
                    Image.is_active == True
                )
            ).all()
            image_ids = [img.id for img in images]
        
        if not image_ids:
            return {
                "success": True,
                "message": "æ²¡æœ‰æ‰¾åˆ°éœ€è¦é‡æ–°åˆ†æçš„å›¾ç‰‡",
                "count": 0
            }
        
        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        db.query(Image).filter(Image.id.in_(image_ids)).update(
            {"ai_analysis_status": "pending"},
            synchronize_session=False
        )
        db.commit()
        
        print(f"ğŸ“Š å°†é‡æ–°åˆ†æ {len(image_ids)} å¼ å›¾ç‰‡")
        
        # å¯åŠ¨æ‰¹é‡åˆ†æä»»åŠ¡
        background_tasks.add_task(
            batch_reanalyze_task,
            image_ids,
            custom_prompt
        )
        
        return {
            "success": True,
            "message": f"å·²å¯åŠ¨æ‰¹é‡é‡æ–°åˆ†æä»»åŠ¡ï¼Œå°†å¤„ç† {len(image_ids)} å¼ å›¾ç‰‡",
            "count": len(image_ids)
        }
        
    except Exception as e:
        print(f"âŒ å¯åŠ¨æ‰¹é‡é‡æ–°åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨æ‰¹é‡é‡æ–°åˆ†æå¤±è´¥: {str(e)}")

@router.delete("/{image_id}")
async def delete_image(
    image_id: int,
    permanent: bool = Query(False, description="æ˜¯å¦æ°¸ä¹…åˆ é™¤"),
    current_user: User = Depends(require_admin),
    db: Session = Depends(get_db)
):
    """åˆ é™¤å›¾ç‰‡"""
    try:
        db_service = DatabaseService(db)
        image = db_service.get_image_by_id(image_id, include_deleted=True)
        
        if not image:
            raise HTTPException(status_code=404, detail="å›¾ç‰‡ä¸å­˜åœ¨")
        
        if permanent:
            # æ°¸ä¹…åˆ é™¤ï¼šåˆ é™¤æ–‡ä»¶å’Œæ•°æ®åº“è®°å½•
            try:
                await storage_manager.delete_image(image.file_path)
            except:
                pass  # æ–‡ä»¶å¯èƒ½å·²ç»ä¸å­˜åœ¨
            
            # åˆ é™¤ç›¸å…³æ ‡ç­¾å…³è”
            db.query(ImageTag).filter(ImageTag.image_id == image_id).delete()
            
            # åˆ é™¤æ•°æ®åº“è®°å½•
            db.delete(image)
        else:
            # è½¯åˆ é™¤
            image.is_active = False
        
        db.commit()
        
        return {
            "success": True,
            "message": "å›¾ç‰‡åˆ é™¤æˆåŠŸ" if permanent else "å›¾ç‰‡å·²ç§»è‡³å›æ”¶ç«™"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å›¾ç‰‡å¤±è´¥: {str(e)}")


@router.post("/batch-update")
async def batch_update_images(
    image_ids: List[int],
    action: str,
    value: Optional[str] = None,
    current_user: User = Depends(require_admin),
    db: Session = Depends(get_db)
):
    """æ‰¹é‡æ›´æ–°å›¾ç‰‡"""
    try:
        updated_count = 0
        db_service = DatabaseService(db)
        
        for image_id in image_ids:
            image = db.query(Image).filter(Image.id == image_id).first()
            if not image:
                continue
                
            if action == "activate":
                image.is_active = True
                updated_count += 1
            elif action == "deactivate":
                image.is_active = False
                updated_count += 1
            elif action == "add_tag" and value:
                # æ·»åŠ æ ‡ç­¾
                db_service.add_tags_to_image(image_id, [value], 'admin', [0.9])
                updated_count += 1
            elif action == "remove_tag" and value:
                # åˆ é™¤æ ‡ç­¾
                db.query(ImageTag).filter(
                    and_(
                        ImageTag.image_id == image_id,
                        ImageTag.tag_id.in_(
                            db.query(Tag.id).filter(Tag.name == value)
                        )
                    )
                ).delete(synchronize_session=False)
                updated_count += 1
            elif action == "delete":
                permanent = value == "permanent"
                if permanent:
                    try:
                        await storage_manager.delete_image(image.file_path)
                    except:
                        pass
                    db.query(ImageTag).filter(ImageTag.image_id == image_id).delete()
                    db.delete(image)
                else:
                    image.is_active = False
                updated_count += 1
        
        db.commit()
        
        return {
            "success": True,
            "message": f"æˆåŠŸæ›´æ–° {updated_count} å¼ å›¾ç‰‡",
            "updated_count": updated_count
        }
        
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡æ›´æ–°å¤±è´¥: {str(e)}")


@router.get("/analytics")
async def get_images_analytics(
    current_user: User = Depends(require_admin),
    db: Session = Depends(get_db)
):
    """è·å–å›¾ç‰‡åˆ†ææ•°æ®"""
    try:
        # æŒ‰ä¸Šä¼ è€…ç»Ÿè®¡
        uploader_stats = db.query(
            Image.uploader,
            func.count(Image.id).label('count'),
            func.avg(Image.ai_confidence).label('avg_confidence'),
            func.sum(Image.file_size).label('total_size')
        ).filter(Image.is_active == True).group_by(Image.uploader).order_by(desc('count')).limit(10).all()
        
        # æŒ‰AIçŠ¶æ€ç»Ÿè®¡
        ai_status_stats = db.query(
            Image.ai_analysis_status,
            func.count(Image.id).label('count')
        ).filter(Image.is_active == True).group_by(Image.ai_analysis_status).all()
        
        # æŒ‰æœˆä»½ç»Ÿè®¡ä¸Šä¼ é‡
        monthly_uploads = db.query(
            func.year(Image.upload_time).label('year'),
            func.month(Image.upload_time).label('month'),
            func.count(Image.id).label('count'),
            func.avg(Image.file_size).label('avg_size')
        ).filter(Image.is_active == True).group_by('year', 'month').order_by('year', 'month').limit(12).all()
        
        # æ ‡ç­¾ä½¿ç”¨é¢‘ç‡
        tag_usage = db.query(
            Tag.name,
            func.count(ImageTag.image_id).label('usage_count'),
            Tag.category
        ).join(ImageTag).join(Image).filter(
            Image.is_active == True
        ).group_by(Tag.name, Tag.category).order_by(desc('usage_count')).limit(30).all()
        
        # æ–‡ä»¶å¤§å°åˆ†å¸ƒ
        size_distribution = db.query(
            func.case([
                (Image.file_size < 100000, 'small'),
                (Image.file_size < 1000000, 'medium'),
                (Image.file_size < 5000000, 'large'),
            ], else_='xlarge').label('size_category'),
            func.count(Image.id).label('count'),
            func.sum(Image.file_size).label('total_size')
        ).filter(Image.is_active == True).group_by('size_category').all()
        
        # å›¾ç‰‡å°ºå¯¸åˆ†å¸ƒ
        dimension_stats = db.query(
            func.case([
                (func.greatest(Image.width, Image.height) < 500, 'small'),
                (func.greatest(Image.width, Image.height) < 1500, 'medium'),
                (func.greatest(Image.width, Image.height) < 3000, 'large'),
            ], else_='ultra').label('dimension_category'),
            func.count(Image.id).label('count'),
            func.avg(Image.width).label('avg_width'),
            func.avg(Image.height).label('avg_height')
        ).filter(Image.is_active == True).group_by('dimension_category').all()
        
        return {
            "success": True,
            "data": {
                "uploader_stats": [
                    {
                        "uploader": stat.uploader,
                        "count": stat.count,
                        "avg_confidence": round(stat.avg_confidence or 0, 2),
                        "total_size": stat.total_size or 0
                    }
                    for stat in uploader_stats
                ],
                "ai_status_stats": [
                    {
                        "status": stat.ai_analysis_status,
                        "count": stat.count
                    }
                    for stat in ai_status_stats
                ],
                "monthly_uploads": [
                    {
                        "year": stat.year,
                        "month": stat.month,
                        "count": stat.count,
                        "avg_size": round(stat.avg_size or 0, 2),
                        "date": f"{stat.year}-{stat.month:02d}"
                    }
                    for stat in monthly_uploads
                ],
                "tag_usage": [
                    {
                        "tag": stat.name,
                        "count": stat.usage_count,
                        "category": stat.category
                    }
                    for stat in tag_usage
                ],
                "size_distribution": [
                    {
                        "category": stat.size_category,
                        "count": stat.count,
                        "total_size": stat.total_size or 0
                    }
                    for stat in size_distribution
                ],
                "dimension_stats": [
                    {
                        "category": stat.dimension_category,
                        "count": stat.count,
                        "avg_width": round(stat.avg_width or 0, 1),
                        "avg_height": round(stat.avg_height or 0, 1)
                    }
                    for stat in dimension_stats
                ]
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å›¾ç‰‡åˆ†ææ•°æ®å¤±è´¥: {str(e)}")


@router.post("/duplicate-check")
async def check_duplicate_images(
    current_user: User = Depends(require_admin),
    db: Session = Depends(get_db)
):
    """æ£€æŸ¥é‡å¤å›¾ç‰‡"""
    try:
        # æŒ‰æ–‡ä»¶åæŸ¥æ‰¾é‡å¤
        filename_duplicates = db.query(
            Image.filename,
            func.count(Image.id).label('count'),
            func.group_concat(Image.id).label('ids')
        ).filter(Image.is_active == True).group_by(Image.filename).having(func.count(Image.id) > 1).all()
        
        # æŒ‰æ–‡ä»¶å¤§å°æŸ¥æ‰¾å¯èƒ½é‡å¤
        size_duplicates = db.query(
            Image.file_size,
            func.count(Image.id).label('count'),
            func.group_concat(Image.id).label('ids')
        ).filter(
            and_(Image.is_active == True, Image.file_size > 0)
        ).group_by(Image.file_size).having(func.count(Image.id) > 1).all()
        
        # æŒ‰å°ºå¯¸æŸ¥æ‰¾å¯èƒ½é‡å¤
        dimension_duplicates = db.query(
            Image.width,
            Image.height,
            func.count(Image.id).label('count'),
            func.group_concat(Image.id).label('ids')
        ).filter(
            and_(Image.is_active == True, Image.width > 0, Image.height > 0)
        ).group_by(Image.width, Image.height).having(func.count(Image.id) > 1).all()
        
        duplicate_data = {
            "filename_duplicates": [
                {
                    "filename": dup.filename,
                    "count": dup.count,
                    "image_ids": [int(id) for id in dup.ids.split(',')]
                }
                for dup in filename_duplicates
            ],
            "size_duplicates": [
                {
                    "file_size": dup.file_size,
                    "count": dup.count,
                    "image_ids": [int(id) for id in dup.ids.split(',')]
                }
                for dup in size_duplicates
            ],
            "dimension_duplicates": [
                {
                    "width": dup.width,
                    "height": dup.height,
                    "count": dup.count,
                    "image_ids": [int(id) for id in dup.ids.split(',')]
                }
                for dup in dimension_duplicates
            ]
        }
        
        return {
            "success": True,
            "data": duplicate_data
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ£€æŸ¥é‡å¤å›¾ç‰‡å¤±è´¥: {str(e)}")


@router.get("/export")
async def export_images_data(
    format: str = Query("csv", description="å¯¼å‡ºæ ¼å¼: csv, json"),
    status: Optional[str] = Query("active"),
    current_user: User = Depends(require_admin),
    db: Session = Depends(get_db)
):
    """å¯¼å‡ºå›¾ç‰‡æ•°æ®"""
    try:
        # æ„å»ºæŸ¥è¯¢
        query = db.query(Image)
        
        if status == "active":
            query = query.filter(Image.is_active == True)
        elif status == "deleted":
            query = query.filter(Image.is_active == False)
        
        images = query.order_by(desc(Image.upload_time)).all()
        
        # è·å–æ¯å¼ å›¾ç‰‡çš„æ ‡ç­¾
        db_service = DatabaseService(db)
        export_data = []
        
        for image in images:
            tags = db_service.get_image_tags(image.id)
            tag_names = [tag.name for tag in tags]
            
            # è§£ææœç´¢å…³é”®è¯
            searchable_keywords = []
            if hasattr(image, 'ai_searchable_keywords') and image.ai_searchable_keywords:
                try:
                    if isinstance(image.ai_searchable_keywords, str):
                        searchable_keywords = json.loads(image.ai_searchable_keywords)
                    else:
                        searchable_keywords = image.ai_searchable_keywords
                except:
                    pass
            
            export_data.append({
                "id": image.id,
                "filename": image.filename,
                "uploader": image.uploader,
                "width": image.width,
                "height": image.height,
                "file_size": image.file_size,
                "description": image.ai_description or "",
                "confidence": image.ai_confidence or 0,
                "ai_status": image.ai_analysis_status,
                "ai_model": getattr(image, 'ai_model', ''),
                "mood": getattr(image, 'ai_mood', ''),
                "style": getattr(image, 'ai_style', ''),
                "tags": ",".join(tag_names),
                "keywords": ",".join(searchable_keywords),
                "upload_time": image.upload_time.isoformat(),
                "view_count": image.view_count,
                "is_active": image.is_active
            })
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if format == "csv":
            output = io.StringIO()
            if export_data:
                writer = csv.DictWriter(output, fieldnames=export_data[0].keys())
                writer.writeheader()
                writer.writerows(export_data)
            
            response = StreamingResponse(
                io.BytesIO(output.getvalue().encode('utf-8-sig')),  # ä½¿ç”¨BOMä»¥æ”¯æŒExcel
                media_type="text/csv",
                headers={"Content-Disposition": f"attachment; filename=images_export_{timestamp}.csv"}
            )
            return response
        
        else:  # JSONæ ¼å¼
            response_data = json.dumps(export_data, ensure_ascii=False, indent=2)
            response = StreamingResponse(
                io.BytesIO(response_data.encode()),
                media_type="application/json",
                headers={"Content-Disposition": f"attachment; filename=images_export_{timestamp}.json"}
            )
            return response
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºæ•°æ®å¤±è´¥: {str(e)}")


# åå°ä»»åŠ¡å‡½æ•° - ä¿®å¤åçš„ç‰ˆæœ¬
async def reanalyze_image_task(image_id: int, file_path: str, custom_prompt: Optional[str] = None):
    """é‡æ–°åˆ†æå›¾ç‰‡çš„åå°ä»»åŠ¡ - ä¿®å¤ç‰ˆæœ¬"""
    db = None
    try:
        print(f"ğŸ”„ å¼€å§‹é‡æ–°åˆ†æå›¾ç‰‡ ID: {image_id}, æ–‡ä»¶è·¯å¾„: {file_path}")
        
        # è·å–å®Œæ•´çš„å›¾ç‰‡URL
        image_url = storage_manager.get_image_url(file_path)
        print(f"ğŸ–¼ï¸ å›¾ç‰‡URL: {image_url}")
        
        # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯æˆ–é»˜è®¤åˆ†æ
        if custom_prompt:
            print(f"ğŸ¤– ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯: {custom_prompt}")
            analysis_result = await gpt4o_analyzer.analyze_with_custom_prompt(image_url, custom_prompt)
        else:
            print(f"ğŸ¤– ä½¿ç”¨é»˜è®¤åˆ†æ")
            analysis_result = await gpt4o_analyzer.analyze_for_search(image_url)
        
        print(f"ğŸ“‹ åˆ†æç»“æœ: {analysis_result}")
        
        # æ”¹è¿›æ•°æ®åº“è¿æ¥å’Œäº‹åŠ¡ç®¡ç†
        from app.database import SessionLocal
        db = SessionLocal()
        
        try:
            db_service = DatabaseService(db)
            image = db_service.get_image_by_id(image_id)
            
            if not image:
                print(f"âŒ å›¾ç‰‡ä¸å­˜åœ¨ ID: {image_id}")
                return
            
            if analysis_result.get("success"):
                analysis = analysis_result["analysis"]
                
                # æ›´æ–°AIåˆ†æç»“æœ
                image.ai_description = analysis.get('description', '')
                image.ai_confidence = analysis.get('confidence', 0.0)
                image.ai_analysis_status = 'completed'
                image.ai_model = 'gpt-4o-reanalyzed' if not custom_prompt else 'gpt-4o-custom'
                
                # å­˜å‚¨å®Œæ•´åˆ†æç»“æœ
                import json
                image.ai_analysis_raw = json.dumps(analysis, ensure_ascii=False)
                image.ai_mood = analysis.get('mood', '')
                image.ai_style = analysis.get('style', '')
                image.ai_searchable_keywords = json.dumps(
                    analysis.get('searchable_keywords', []), 
                    ensure_ascii=False
                )
                
                # åˆ†æ­¥å¤„ç†æ ‡ç­¾æ“ä½œ
                try:
                    # 1. å…ˆåˆ é™¤æ—§æ ‡ç­¾
                    from app.models.image import ImageTag
                    deleted_count = db.query(ImageTag).filter(ImageTag.image_id == image_id).delete()
                    print(f"ğŸ—‘ï¸ åˆ é™¤äº† {deleted_count} ä¸ªæ—§æ ‡ç­¾")
                    
                    # 2. æäº¤åˆ é™¤æ“ä½œ
                    db.flush()
                    
                    # 3. æ·»åŠ æ–°æ ‡ç­¾
                    await _process_reanalyzed_tags_safe(db_service, image_id, analysis)
                    
                    # 4. æäº¤æ‰€æœ‰æ›´æ”¹
                    db.commit()
                    print(f"âœ… é‡æ–°åˆ†æå®Œæˆ ID: {image_id}")
                    
                except Exception as tag_error:
                    print(f"âŒ æ ‡ç­¾å¤„ç†å¤±è´¥: {tag_error}")
                    db.rollback()
                    # æ›´æ–°ä¸ºå¤±è´¥çŠ¶æ€
                    image.ai_analysis_status = 'failed'
                    db.commit()
                    
            else:
                image.ai_analysis_status = 'failed'
                db.commit()
                print(f"âŒ é‡æ–°åˆ†æå¤±è´¥ ID: {image_id}, é”™è¯¯: {analysis_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
        except Exception as db_error:
            print(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {db_error}")
            if db:
                db.rollback()
            raise
            
    except Exception as e:
        print(f"âŒ é‡æ–°åˆ†æä»»åŠ¡å¤±è´¥: {e}")
        # ç¡®ä¿è®¾ç½®å¤±è´¥çŠ¶æ€
        if db:
            try:
                image = db.query(Image).filter(Image.id == image_id).first()
                if image:
                    image.ai_analysis_status = 'failed'
                    db.commit()
            except Exception as fallback_e:
                print(f"âŒ è®¾ç½®å¤±è´¥çŠ¶æ€å¤±è´¥: {fallback_e}")
                
    finally:
        if db:
            db.close()


async def _process_reanalyzed_tags_safe(db_service: DatabaseService, image_id: int, analysis: dict):
    """å®‰å…¨åœ°å¤„ç†é‡æ–°åˆ†æçš„æ ‡ç­¾"""
    try:
        tags = analysis.get('tags', {})
        all_tags = []
        confidences = []
        
        # å¤„ç†åˆ†ç±»æ ‡ç­¾
        for category, tag_list in tags.items():
            if isinstance(tag_list, list):
                for tag in tag_list:
                    if tag and tag.strip() and tag not in all_tags:
                        all_tags.append(tag.strip())
                        confidences.append(analysis.get('confidence', 0.8))
        
        # æ·»åŠ æœç´¢å…³é”®è¯ä½œä¸ºæ ‡ç­¾
        searchable_keywords = analysis.get('searchable_keywords', [])
        for keyword in searchable_keywords:
            if keyword and keyword.strip() and keyword not in all_tags:
                all_tags.append(keyword.strip())
                confidences.append(analysis.get('confidence', 0.8))
        
        # æ·»åŠ æ°›å›´å’Œé£æ ¼ä½œä¸ºæ ‡ç­¾
        mood = analysis.get('mood', '')
        style = analysis.get('style', '')
        
        if mood and mood not in all_tags:
            all_tags.append(mood)
            confidences.append(analysis.get('confidence', 0.7))
        
        if style and style not in all_tags:
            all_tags.append(style)
            confidences.append(analysis.get('confidence', 0.7))
        
        # åˆ†æ‰¹æ·»åŠ æ ‡ç­¾ï¼Œé¿å…ä¸€æ¬¡æ€§æ“ä½œå¤ªå¤š
        if all_tags:
            batch_size = 10
            for i in range(0, len(all_tags), batch_size):
                batch_tags = all_tags[i:i + batch_size]
                batch_confidences = confidences[i:i + batch_size]
                
                try:
                    db_service.add_tags_to_image_safe(image_id, batch_tags, 'gpt4o-reanalyzed', batch_confidences)
                    db_service.db.flush()  # ç«‹å³æ‰§è¡Œ
                    print(f"âœ… æˆåŠŸæ·»åŠ æ ‡ç­¾æ‰¹æ¬¡: {len(batch_tags)} ä¸ª")
                except Exception as batch_error:
                    print(f"âŒ æ·»åŠ æ ‡ç­¾æ‰¹æ¬¡å¤±è´¥: {batch_error}")
                    raise
            
            print(f"âœ… æ€»å…±æˆåŠŸå¤„ç† {len(all_tags)} ä¸ªæ ‡ç­¾")
            
    except Exception as e:
        print(f"âŒ å¤„ç†æ ‡ç­¾å¤±è´¥: {e}")
        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†


# ä¿ç•™åŸæœ‰çš„ _process_reanalyzed_tags å‡½æ•°ä½œä¸ºå¤‡ç”¨
async def _process_reanalyzed_tags(db_service: DatabaseService, image_id: int, analysis: dict):
    """å¤„ç†é‡æ–°åˆ†æçš„æ ‡ç­¾ - åŸç‰ˆæœ¬"""
    try:
        tags = analysis.get('tags', {})
        all_tags = []
        confidences = []
        
        # å¤„ç†åˆ†ç±»æ ‡ç­¾
        for category, tag_list in tags.items():
            if isinstance(tag_list, list):
                for tag in tag_list:
                    if tag and tag.strip() and tag not in all_tags:
                        all_tags.append(tag.strip())
                        confidences.append(analysis.get('confidence', 0.8))
        
        # æ·»åŠ æœç´¢å…³é”®è¯ä½œä¸ºæ ‡ç­¾
        searchable_keywords = analysis.get('searchable_keywords', [])
        for keyword in searchable_keywords:
            if keyword and keyword.strip() and keyword not in all_tags:
                all_tags.append(keyword.strip())
                confidences.append(analysis.get('confidence', 0.8))
        
        # æ·»åŠ æ°›å›´å’Œé£æ ¼ä½œä¸ºæ ‡ç­¾
        mood = analysis.get('mood', '')
        style = analysis.get('style', '')
        
        if mood and mood not in all_tags:
            all_tags.append(mood)
            confidences.append(analysis.get('confidence', 0.7))
        
        if style and style not in all_tags:
            all_tags.append(style)
            confidences.append(analysis.get('confidence', 0.7))
        
        # æ·»åŠ åˆ°æ•°æ®åº“
        if all_tags:
            db_service.add_tags_to_image(image_id, all_tags, 'gpt4o-reanalyzed', confidences)
            print(f"âœ… æˆåŠŸå¤„ç† {len(all_tags)} ä¸ªæ ‡ç­¾")
            
    except Exception as e:
        print(f"âŒ å¤„ç†æ ‡ç­¾å¤±è´¥: {e}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚ç»§ç»­å¤„ç†